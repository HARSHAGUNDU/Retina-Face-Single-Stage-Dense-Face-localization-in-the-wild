{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75593b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import argparse\n",
    "import torch.utils.data as data\n",
    "from data import WiderFaceDetection, detection_collate, preproc\n",
    "from layers.modules import MultiBoxLoss\n",
    "from layers.functions.prior_box import PriorBox\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "from models.retinaface import RetinaFace\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cbccefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models.detection.backbone_utils as backbone_utils\n",
    "import torchvision.models._utils as _utils\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "from models.net import MobileNetV1 as MobileNetV1\n",
    "from models.net import FPN as FPN\n",
    "from models.net import SSH as SSH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f0e727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_mnet = {\n",
    "    'name': 'mobilenet0.25',\n",
    "    'min_sizes': [[16, 32], [64, 128], [256, 512]],\n",
    "    'steps': [8, 16, 32],\n",
    "    'variance': [0.1, 0.2],\n",
    "    'clip': False,\n",
    "    'loc_weight': 2.0,\n",
    "    'gpu_train': True,\n",
    "    'batch_size': 32,\n",
    "    'ngpu': 1,\n",
    "    'epoch': 250,\n",
    "    'decay1': 190,\n",
    "    'decay2': 220,\n",
    "    'image_size': 640,\n",
    "    'pretrain': True,\n",
    "    'return_layers': {'stage1': 1, 'stage2': 2, 'stage3': 3},\n",
    "    'in_channel': 32,\n",
    "    'out_channel': 64\n",
    "}\n",
    "\n",
    "cfg_re50 = {\n",
    "    'name': 'Resnet50',\n",
    "    'min_sizes': [[16, 32], [64, 128], [256, 512]],\n",
    "    'steps': [8, 16, 32],\n",
    "    'variance': [0.1, 0.2],\n",
    "    'clip': False,\n",
    "    'loc_weight': 2.0,\n",
    "    'gpu_train': True,\n",
    "    'batch_size': 24,\n",
    "    'ngpu': 4,\n",
    "    'epoch': 100,\n",
    "    'decay1': 70,\n",
    "    'decay2': 90,\n",
    "    'image_size': 840,\n",
    "    'pretrain': True,\n",
    "    'return_layers': {'layer2': 1, 'layer3': 2, 'layer4': 3},\n",
    "    'in_channel': 256,\n",
    "    'out_channel': 256\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91e15909",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_mean = (104, 117, 123) # bgr order\n",
    "num_classes = 2\n",
    "\n",
    "num_workers = 4\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "initial_lr = 1e-3\n",
    "gamma = 0.1\n",
    "training_dataset = \"data/dataset/train/label.txt\"\n",
    "save_folder = './weights/'\n",
    "max_epoch = 10\n",
    "network = 'mobile0.25'\n",
    "resume_net = None\n",
    "resume_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccd5259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if network == \"mobile0.25\":\n",
    "    cfg = cfg_mnet\n",
    "elif network == \"resnet50\":\n",
    "    cfg = cfg_re50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b63feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dim = cfg['image_size']\n",
    "num_gpu = cfg['ngpu']\n",
    "batch_size = cfg['batch_size']\n",
    "gpu_train = cfg['gpu_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae4326c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataFaceDetection(data.Dataset):\n",
    "    def __init__(self, txt_path, preproc=None):\n",
    "        self.preproc = preproc\n",
    "        self.imgs_path = []\n",
    "        self.words = []\n",
    "        f = open(txt_path,'r')\n",
    "        lines = f.readlines()\n",
    "        isFirst = True\n",
    "        labels = []\n",
    "        for line in lines:\n",
    "            line = line.rstrip()\n",
    "            if line.startswith('#'):\n",
    "                if isFirst is True:\n",
    "                    isFirst = False\n",
    "                else:\n",
    "                    labels_copy = labels.copy()\n",
    "                    self.words.append(labels_copy)\n",
    "                    labels.clear()\n",
    "                path = line[2:]\n",
    "                path = txt_path.replace('label.txt','images/') + path\n",
    "                self.imgs_path.append(path)\n",
    "            else:\n",
    "                line = line.split(' ')\n",
    "                label = [float(x) for x in line]\n",
    "                labels.append(label)\n",
    "\n",
    "        self.words.append(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = cv2.imread(self.imgs_path[index])\n",
    "        height, width, _ = img.shape\n",
    "\n",
    "        labels = self.words[index]\n",
    "        annotations = np.zeros((0, 15))\n",
    "        if len(labels) == 0:\n",
    "            return annotations\n",
    "        for idx, label in enumerate(labels):\n",
    "            annotation = np.zeros((1, 15))\n",
    "            # bbox\n",
    "            annotation[0, 0] = label[0]  # x1\n",
    "            annotation[0, 1] = label[1]  # y1\n",
    "            annotation[0, 2] = label[0] + label[2]  # x2\n",
    "            annotation[0, 3] = label[1] + label[3]  # y2\n",
    "\n",
    "            # landmarks\n",
    "            annotation[0, 4] = label[4]    # l0_x\n",
    "            annotation[0, 5] = label[5]    # l0_y\n",
    "            annotation[0, 6] = label[7]    # l1_x\n",
    "            annotation[0, 7] = label[8]    # l1_y\n",
    "            annotation[0, 8] = label[10]   # l2_x\n",
    "            annotation[0, 9] = label[11]   # l2_y\n",
    "            annotation[0, 10] = label[13]  # l3_x\n",
    "            annotation[0, 11] = label[14]  # l3_y\n",
    "            annotation[0, 12] = label[16]  # l4_x\n",
    "            annotation[0, 13] = label[17]  # l4_y\n",
    "            if (annotation[0, 4]<0):\n",
    "                annotation[0, 14] = -1\n",
    "            else:\n",
    "                annotation[0, 14] = 1\n",
    "\n",
    "            annotations = np.append(annotations, annotation, axis=0)\n",
    "        target = np.array(annotations)\n",
    "        if self.preproc is not None:\n",
    "            img, target = self.preproc(img, target)\n",
    "\n",
    "        return torch.from_numpy(img), target\n",
    "\n",
    "def detection_collate(batch):\n",
    "    \"\"\"Custom collate fn for dealing with batches of images that have a different\n",
    "    number of associated object annotations (bounding boxes).\n",
    "\n",
    "    Arguments:\n",
    "        batch: (tuple) A tuple of tensor images and lists of annotations\n",
    "\n",
    "    Return:\n",
    "        A tuple containing:\n",
    "            1) (tensor) batch of images stacked on their 0 dim\n",
    "            2) (list of tensors) annotations for a given image are stacked on 0 dim\n",
    "    \"\"\"\n",
    "    targets = []\n",
    "    imgs = []\n",
    "    for _, sample in enumerate(batch):\n",
    "        for _, tup in enumerate(sample):\n",
    "            if torch.is_tensor(tup):\n",
    "                imgs.append(tup)\n",
    "            elif isinstance(tup, type(np.empty(0))):\n",
    "                annos = torch.from_numpy(tup).float()\n",
    "                targets.append(annos)\n",
    "\n",
    "    return (torch.stack(imgs, 0), targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e727c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing net...\n",
      "RetinaFace(\n",
      "  (body): IntermediateLayerGetter(\n",
      "    (stage1): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
      "        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
      "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (stage2): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (stage3): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fpn): FPN(\n",
      "    (output1): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (output2): Sequential(\n",
      "      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (output3): Sequential(\n",
      "      (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (merge1): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (merge2): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (ssh1): SSH(\n",
      "    (conv3X3): Sequential(\n",
      "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (conv5X5_1): Sequential(\n",
      "      (0): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (conv5X5_2): Sequential(\n",
      "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (conv7X7_2): Sequential(\n",
      "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (conv7x7_3): Sequential(\n",
      "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (ssh2): SSH(\n",
      "    (conv3X3): Sequential(\n",
      "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (conv5X5_1): Sequential(\n",
      "      (0): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (conv5X5_2): Sequential(\n",
      "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (conv7X7_2): Sequential(\n",
      "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (conv7x7_3): Sequential(\n",
      "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (ssh3): SSH(\n",
      "    (conv3X3): Sequential(\n",
      "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (conv5X5_1): Sequential(\n",
      "      (0): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (conv5X5_2): Sequential(\n",
      "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (conv7X7_2): Sequential(\n",
      "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (conv7x7_3): Sequential(\n",
      "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (ClassHead): ModuleList(\n",
      "    (0): ClassHead(\n",
      "      (conv1x1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (1): ClassHead(\n",
      "      (conv1x1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (2): ClassHead(\n",
      "      (conv1x1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (BboxHead): ModuleList(\n",
      "    (0): BboxHead(\n",
      "      (conv1x1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (1): BboxHead(\n",
      "      (conv1x1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (2): BboxHead(\n",
      "      (conv1x1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (LandmarkHead): ModuleList(\n",
      "    (0): LandmarkHead(\n",
      "      (conv1x1): Conv2d(64, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (1): LandmarkHead(\n",
      "      (conv1x1): Conv2d(64, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (2): LandmarkHead(\n",
      "      (conv1x1): Conv2d(64, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = RetinaFace(cfg=cfg)\n",
    "print(\"Printing net...\")\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc9bd358",
   "metadata": {},
   "outputs": [],
   "source": [
    "if resume_net is not None:\n",
    "    print('Loading resume network...')\n",
    "    state_dict = torch.load(resume_net)\n",
    "    # create new OrderedDict that does not contain `module.`\n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        head = k[:7]\n",
    "        if head == 'module.':\n",
    "            name = k[7:] # remove `module.`\n",
    "        else:\n",
    "            name = k\n",
    "        new_state_dict[name] = v\n",
    "    net.load_state_dict(new_state_dict)\n",
    "\n",
    "if num_gpu > 1 and gpu_train:\n",
    "    net = torch.nn.DataParallel(net).cuda()\n",
    "else:\n",
    "    net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "296642d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=initial_lr, momentum=momentum, weight_decay=weight_decay)\n",
    "criterion = MultiBoxLoss(num_classes, 0.35, True, 0, True, 7, 0.35, False)\n",
    "\n",
    "priorbox = PriorBox(cfg, image_size=(img_dim, img_dim))\n",
    "with torch.no_grad():\n",
    "    priors = priorbox.forward()\n",
    "    priors = priors.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c37765f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, gamma, epoch, step_index, iteration, epoch_size):\n",
    "    \"\"\"Sets the learning rate\n",
    "    # Adapted from PyTorch Imagenet example:\n",
    "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    warmup_epoch = -1\n",
    "    if epoch <= warmup_epoch:\n",
    "        lr = 1e-6 + (initial_lr-1e-6) * iteration / (epoch_size * warmup_epoch)\n",
    "    else:\n",
    "        lr = initial_lr * (gamma ** (step_index))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5cf12b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    net.train()\n",
    "    epoch = 0 + resume_epoch\n",
    "    print('Loading Dataset...')\n",
    "\n",
    "    dataset = DataFaceDetection( training_dataset,preproc(img_dim, rgb_mean))\n",
    "\n",
    "    epoch_size = math.ceil(len(dataset) / batch_size)\n",
    "    max_iter = max_epoch * epoch_size\n",
    "\n",
    "    stepvalues = (cfg['decay1'] * epoch_size, cfg['decay2'] * epoch_size)\n",
    "    step_index = 0\n",
    "\n",
    "    if resume_epoch > 0:\n",
    "        start_iter = resume_epoch * epoch_size\n",
    "    else:\n",
    "        start_iter = 0\n",
    "\n",
    "    for iteration in range(start_iter, max_iter):\n",
    "        if iteration % epoch_size == 0:\n",
    "            # create batch iterator\n",
    "            batch_iterator = iter(data.DataLoader(dataset, batch_size, shuffle=True, num_workers=num_workers, collate_fn=detection_collate))\n",
    "            if (epoch % 10 == 0 and epoch > 0) or (epoch % 5 == 0 and epoch > cfg['decay1']):\n",
    "                torch.save(net.state_dict(), save_folder + cfg['name']+ '_epoch_' + str(epoch) + '.pth')\n",
    "            epoch += 1\n",
    "\n",
    "        load_t0 = time.time()\n",
    "        if iteration in stepvalues:\n",
    "            step_index += 1\n",
    "        lr = adjust_learning_rate(optimizer, gamma, epoch, step_index, iteration, epoch_size)\n",
    "\n",
    "        # load train data\n",
    "        images, targets = next(batch_iterator)\n",
    "        images = images.cuda()\n",
    "        targets = [anno.cuda() for anno in targets]\n",
    "\n",
    "        # forward\n",
    "        out = net(images)\n",
    "\n",
    "        #print (\"Outputs from the model \\n\", out)\n",
    "        print (\"Outputs from the model \\n\", len (out[0]), len (out[0][0]), out[0][0].shape, out[0][0][1].shape)\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss_l, loss_c, loss_landm = criterion(out, priors, targets)\n",
    "        loss = cfg['loc_weight'] * loss_l + loss_c + loss_landm\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        load_t1 = time.time()\n",
    "        batch_time = load_t1 - load_t0\n",
    "        eta = int(batch_time * (max_iter - iteration))\n",
    "        print('Epoch:{}/{} || Epochiter: {}/{} || Iter: {}/{} || Loc: {:.4f} Cla: {:.4f} Landm: {:.4f} || LR: {:.8f} || Batchtime: {:.4f} s || ETA: {}'\n",
    "              .format(epoch, max_epoch, (iteration % epoch_size) + 1,\n",
    "              epoch_size, iteration + 1, max_iter, loss_l.item(), loss_c.item(), loss_landm.item(), lr, batch_time, str(datetime.timedelta(seconds=eta))))\n",
    "\n",
    "    print (\"model save path : \", save_folder + cfg['name'] + '_Final.pth')\n",
    "    torch.save(net.state_dict(), save_folder + cfg['name'] + '_Final.pth')\n",
    "    # torch.save(net.state_dict(), save_folder + 'Final_Retinaface.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7d623e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset...\n",
      "Outputs from the model \n",
      " 16 16800 torch.Size([16800, 4]) torch.Size([4])\n",
      "Epoch:1/10 || Epochiter: 1/1 || Iter: 1/10 || Loc: 4.1830 Cla: 13.3764 Landm: 18.7772 || LR: 0.00100000 || Batchtime: 2.2613 s || ETA: 0:00:22\n",
      "Outputs from the model \n",
      " 16 16800 torch.Size([16800, 4]) torch.Size([4])\n",
      "Epoch:2/10 || Epochiter: 1/1 || Iter: 2/10 || Loc: 4.3896 Cla: 12.3880 Landm: 18.5959 || LR: 0.00100000 || Batchtime: 0.3734 s || ETA: 0:00:03\n",
      "Outputs from the model \n",
      " 16 16800 torch.Size([16800, 4]) torch.Size([4])\n",
      "Epoch:3/10 || Epochiter: 1/1 || Iter: 3/10 || Loc: 4.3152 Cla: 11.4215 Landm: 18.2886 || LR: 0.00100000 || Batchtime: 0.4071 s || ETA: 0:00:03\n",
      "Outputs from the model \n",
      " 16 16800 torch.Size([16800, 4]) torch.Size([4])\n",
      "Epoch:4/10 || Epochiter: 1/1 || Iter: 4/10 || Loc: 4.1519 Cla: 10.1791 Landm: 18.0470 || LR: 0.00100000 || Batchtime: 0.3086 s || ETA: 0:00:02\n",
      "Outputs from the model \n",
      " 16 16800 torch.Size([16800, 4]) torch.Size([4])\n",
      "Epoch:5/10 || Epochiter: 1/1 || Iter: 5/10 || Loc: 4.2185 Cla: 9.3281 Landm: 18.5446 || LR: 0.00100000 || Batchtime: 0.3264 s || ETA: 0:00:01\n",
      "Outputs from the model \n",
      " 16 16800 torch.Size([16800, 4]) torch.Size([4])\n",
      "Epoch:6/10 || Epochiter: 1/1 || Iter: 6/10 || Loc: 4.3416 Cla: 8.3555 Landm: 17.4893 || LR: 0.00100000 || Batchtime: 0.3068 s || ETA: 0:00:01\n",
      "Outputs from the model \n",
      " 16 16800 torch.Size([16800, 4]) torch.Size([4])\n",
      "Epoch:7/10 || Epochiter: 1/1 || Iter: 7/10 || Loc: 3.8451 Cla: 7.8858 Landm: 17.4999 || LR: 0.00100000 || Batchtime: 0.3262 s || ETA: 0:00:01\n",
      "Outputs from the model \n",
      " 16 16800 torch.Size([16800, 4]) torch.Size([4])\n",
      "Epoch:8/10 || Epochiter: 1/1 || Iter: 8/10 || Loc: 3.9765 Cla: 7.2421 Landm: 17.0891 || LR: 0.00100000 || Batchtime: 0.3502 s || ETA: 0:00:01\n",
      "Outputs from the model \n",
      " 16 16800 torch.Size([16800, 4]) torch.Size([4])\n",
      "Epoch:9/10 || Epochiter: 1/1 || Iter: 9/10 || Loc: 3.3742 Cla: 7.2124 Landm: 16.8960 || LR: 0.00100000 || Batchtime: 0.3573 s || ETA: 0:00:00\n",
      "Outputs from the model \n",
      " 16 16800 torch.Size([16800, 4]) torch.Size([4])\n",
      "Epoch:10/10 || Epochiter: 1/1 || Iter: 10/10 || Loc: 3.6595 Cla: 6.7702 Landm: 16.4701 || LR: 0.00100000 || Batchtime: 0.3303 s || ETA: 0:00:00\n",
      "model save path :  ./weights/mobilenet0.25_Final.pth\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "59f076dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_size=True\n",
    "dataset_folder = './data/dataset/val/images/'\n",
    "nms_threshold = 0.5\n",
    "save_folder = './output_results'\n",
    "confidence_threshold = 0.02\n",
    "vis_thres = 0.5\n",
    "save_image = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f6d3659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "from data import cfg_mnet, cfg_re50\n",
    "from layers.functions.prior_box import PriorBox\n",
    "from utils.nms.py_cpu_nms import py_cpu_nms\n",
    "import cv2\n",
    "from models.retinaface import RetinaFace\n",
    "from utils.box_utils import decode, decode_landm\n",
    "from utils.timer import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2fa88590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data():\n",
    "    torch.set_grad_enabled(False)\n",
    "    \n",
    "    device = torch.device(\"cpu\" if 0 else \"cuda\")\n",
    "    net.eval()\n",
    "    #print('Finished loading model!')\n",
    "    #print(net)\n",
    "    #cudnn.benchmark = True\n",
    "    #device = torch.device(\"cpu\" if cpu else \"cuda\")\n",
    "    #net = net.to(device)\n",
    "\n",
    "    # testing dataset\n",
    "    testset_folder = dataset_folder\n",
    "    testset_list = dataset_folder[:-7] + \"label.txt\"\n",
    "\n",
    "    with open(testset_list, 'r') as fr:\n",
    "        test_dataset = fr.read().split()\n",
    "    num_images = len(test_dataset)\n",
    "\n",
    "    _t = {'forward_pass': Timer(), 'misc': Timer()}\n",
    "\n",
    "    # testing begin\n",
    "    for i, img_name in enumerate(test_dataset):\n",
    "        image_path = testset_folder + img_name\n",
    "        img_raw = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        img = np.float32(img_raw)\n",
    "\n",
    "        # testing scale\n",
    "        target_size = 1600\n",
    "        max_size = 2150\n",
    "        im_shape = img.shape\n",
    "        im_size_min = np.min(im_shape[0:2])\n",
    "        im_size_max = np.max(im_shape[0:2])\n",
    "        resize = float(target_size) / float(im_size_min)\n",
    "        # prevent bigger axis from being more than max_size:\n",
    "        if np.round(resize * im_size_max) > max_size:\n",
    "            resize = float(max_size) / float(im_size_max)\n",
    "        if origin_size:\n",
    "            resize = 1\n",
    "\n",
    "        if resize != 1:\n",
    "            img = cv2.resize(img, None, None, fx=resize, fy=resize, interpolation=cv2.INTER_LINEAR)\n",
    "        im_height, im_width, _ = img.shape\n",
    "        scale = torch.Tensor([img.shape[1], img.shape[0], img.shape[1], img.shape[0]])\n",
    "        img -= (104, 117, 123)\n",
    "        img = img.transpose(2, 0, 1)\n",
    "        img = torch.from_numpy(img).unsqueeze(0)\n",
    "        img = img.to(device)\n",
    "        scale = scale.to(device)\n",
    "\n",
    "        _t['forward_pass'].tic()\n",
    "        loc, conf, landms = net(img)  # forward pass\n",
    "        _t['forward_pass'].toc()\n",
    "        _t['misc'].tic()\n",
    "        priorbox = PriorBox(cfg, image_size=(im_height, im_width))\n",
    "        priors = priorbox.forward()\n",
    "        priors = priors.to(device)\n",
    "        prior_data = priors.data\n",
    "        boxes = decode(loc.data.squeeze(0), prior_data, cfg['variance'])\n",
    "        boxes = boxes * scale / resize\n",
    "        boxes = boxes.cpu().numpy()\n",
    "        scores = conf.squeeze(0).data.cpu().numpy()[:, 1]\n",
    "        landms = decode_landm(landms.data.squeeze(0), prior_data, cfg['variance'])\n",
    "        scale1 = torch.Tensor([img.shape[3], img.shape[2], img.shape[3], img.shape[2],\n",
    "                               img.shape[3], img.shape[2], img.shape[3], img.shape[2],\n",
    "                               img.shape[3], img.shape[2]])\n",
    "        scale1 = scale1.to(device)\n",
    "        landms = landms * scale1 / resize\n",
    "        landms = landms.cpu().numpy()\n",
    "\n",
    "        # ignore low scores\n",
    "        inds = np.where(scores > confidence_threshold)[0]\n",
    "        boxes = boxes[inds]\n",
    "        landms = landms[inds]\n",
    "        scores = scores[inds]\n",
    "\n",
    "        # keep top-K before NMS\n",
    "        order = scores.argsort()[::-1]\n",
    "        # order = scores.argsort()[::-1][:top_k]\n",
    "        boxes = boxes[order]\n",
    "        landms = landms[order]\n",
    "        scores = scores[order]\n",
    "\n",
    "        # do NMS\n",
    "        dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)\n",
    "        keep = py_cpu_nms(dets, nms_threshold)\n",
    "        # keep = nms(dets, nms_threshold,force_cpu=cpu)\n",
    "        dets = dets[keep, :]\n",
    "        landms = landms[keep]\n",
    "\n",
    "        # keep top-K faster NMS\n",
    "        # dets = dets[:keep_top_k, :]\n",
    "        # landms = landms[:keep_top_k, :]\n",
    "\n",
    "        dets = np.concatenate((dets, landms), axis=1)\n",
    "        _t['misc'].toc()\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        save_name = save_folder + img_name[:-4] + \".txt\"\n",
    "        dirname = os.path.dirname(save_name)\n",
    "        if not os.path.isdir(dirname):\n",
    "            os.makedirs(dirname)\n",
    "        print (\"save_name \", save_name)\n",
    "        print (dets)\n",
    "        with open(save_name, \"w\") as fd:\n",
    "            bboxs = dets\n",
    "            file_name = os.path.basename(save_name)[:-4] + \"\\n\"\n",
    "            bboxs_num = str(len(bboxs)) + \"\\n\"\n",
    "            fd.write(file_name)\n",
    "            fd.write(bboxs_num)\n",
    "            for box in bboxs:\n",
    "                x = int(box[0])\n",
    "                y = int(box[1])\n",
    "                w = int(box[2]) - int(box[0])\n",
    "                h = int(box[3]) - int(box[1])\n",
    "                confidence = str(box[4])\n",
    "                line = str(x) + \" \" + str(y) + \" \" + str(w) + \" \" + str(h) + \" \" + confidence + \" \\n\"\n",
    "                fd.write(line)\n",
    "\n",
    "        print('im_detect: {:d}/{:d} forward_pass_time: {:.4f}s misc: {:.4f}s'.format(i + 1, num_images, _t['forward_pass'].average_time, _t['misc'].average_time))\n",
    "\n",
    "        # save image\n",
    "        if save_image:\n",
    "            for b in dets:\n",
    "                if b[4] < vis_thres:\n",
    "                    continue\n",
    "                text = \"{:.4f}\".format(b[4])\n",
    "                b = list(map(int, b))\n",
    "                cv2.rectangle(img_raw, (b[0], b[1]), (b[2], b[3]), (0, 0, 255), 2)\n",
    "                cx = b[0]\n",
    "                cy = b[1] + 12\n",
    "                cv2.putText(img_raw, text, (cx, cy),\n",
    "                            cv2.FONT_HERSHEY_DUPLEX, 0.5, (255, 255, 255))\n",
    "\n",
    "                # landms\n",
    "                cv2.circle(img_raw, (b[5], b[6]), 1, (0, 0, 255), 4)\n",
    "                cv2.circle(img_raw, (b[7], b[8]), 1, (0, 255, 255), 4)\n",
    "                cv2.circle(img_raw, (b[9], b[10]), 1, (255, 0, 255), 4)\n",
    "                cv2.circle(img_raw, (b[11], b[12]), 1, (0, 255, 0), 4)\n",
    "                cv2.circle(img_raw, (b[13], b[14]), 1, (255, 0, 0), 4)\n",
    "            # save image\n",
    "            if not os.path.exists(save_folder):\n",
    "                os.makedirs(save_folder)\n",
    "            name = os.path.join (save_folder , str(i) + \".jpg\")\n",
    "            print (name)\n",
    "            cv2.imwrite(name, img_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01458de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_name  ./output_resultspic17.txt\n",
      "[[ 57.2387    87.951096 119.69016  ... 120.96333   87.090004 120.31213 ]\n",
      " [ 73.27296   72.45918  136.04187  ... 104.99694  103.573616 104.149   ]\n",
      " [105.118614  72.16471  167.82964  ... 104.2515   135.59097  103.93772 ]\n",
      " ...\n",
      " [ 68.60593  143.70444  199.56264  ... 217.54892  138.8873   216.54335 ]\n",
      " [ 11.999426 196.09796   27.826164 ... 203.77562   19.973005 204.20065 ]\n",
      " [ 35.69064  188.08946   52.008785 ... 195.58926   43.859333 196.23999 ]]\n",
      "im_detect: 1/6 forward_pass_time: 0.0051s misc: 0.0051s\n",
      "./output_results/0.jpg\n",
      "save_name  ./output_resultspic18.txt\n",
      "[[ 57.260212   71.302124  117.73827   ... 105.050476   86.123245\n",
      "  104.61233  ]\n",
      " [ 26.45527   167.63242    87.06356   ... 200.02393    55.4319\n",
      "  199.30637  ]\n",
      " [ 56.6758    105.3215    117.6093    ... 136.69638    86.964935\n",
      "  136.21255  ]\n",
      " ...\n",
      " [ 52.3997    196.36295    68.22402   ... 204.05309    60.048405\n",
      "  204.18776  ]\n",
      " [ -4.0566764  52.037884   11.831428  ...  59.773693    3.995819\n",
      "   60.15825  ]\n",
      " [ -4.158444   92.045944   11.743327  ...  99.73612     3.9222348\n",
      "  100.21667  ]]\n",
      "im_detect: 2/6 forward_pass_time: 0.0049s misc: 0.0045s\n",
      "./output_results/1.jpg\n",
      "save_name  ./output_resultsotoole-families.txt\n",
      "[[151.96588  104.35267  214.15828  ... 136.96014  183.73335  135.52489 ]\n",
      " [169.11165  120.387405 230.99364  ... 152.81784  199.03848  152.2199  ]\n",
      " [ 76.67893  -87.40189  559.53784  ... 149.54887  300.46863  147.16527 ]\n",
      " ...\n",
      " [219.84833  276.07516  235.87192  ... 283.5883   227.91971  284.2404  ]\n",
      " [155.91571  260.12808  171.83806  ... 267.6763   163.89203  268.21036 ]\n",
      " [235.96169  228.02412  251.92802  ... 235.78409  244.01239  236.19933 ]]\n",
      "im_detect: 3/6 forward_pass_time: 0.0048s misc: 0.0073s\n",
      "./output_results/2.jpg\n",
      "save_name  ./output_resultsotoole-chargers.txt\n",
      "[[663.0579    71.303566 724.44556  ... 104.32977  695.3588   104.90736 ]\n",
      " [350.17007  -32.568836 490.10788  ...  41.690754 429.61096   40.98943 ]\n",
      " [290.65128   48.82999  424.05368  ... 124.05179  359.4968   119.671196]\n",
      " ...\n",
      " [563.87726  340.0863   580.0471   ... 347.6653   572.0033   348.14    ]\n",
      " [419.90808  387.9551   435.8811   ... 395.71457  428.01157  396.13483 ]\n",
      " [771.8189   324.05826  787.9825   ... 331.7447   780.0404   332.19388 ]]\n",
      "im_detect: 4/6 forward_pass_time: 0.0047s misc: 0.0135s\n",
      "./output_results/3.jpg\n",
      "save_name  ./output_resultsmia-rodriguez.txt\n",
      "[[650.1803   343.37512  710.5389   ... 377.25928  678.7602   376.73825 ]\n",
      " [632.54767  359.44794  695.1243   ... 393.17776  663.0678   391.84885 ]\n",
      " [617.1145   343.0254   678.75964  ... 377.11432  647.8179   376.84726 ]\n",
      " ...\n",
      " [491.94046  243.94688  507.85632  ... 251.83138  500.02542  252.17662 ]\n",
      " [243.89021   35.87544  260.0677   ...  43.755173 251.87852   44.126957]\n",
      " [451.75223  412.03397  467.84357  ... 419.77426  459.9634   420.22968 ]]\n",
      "im_detect: 5/6 forward_pass_time: 0.0048s misc: 0.0189s\n",
      "./output_results/4.jpg\n",
      "save_name  ./output_resultsprato-family-dinner752.txt\n",
      "[[314.71793  119.43783  375.8547   ... 153.45439  344.36462  152.07256 ]\n",
      " [248.5579   119.2164   309.68008  ... 151.51935  280.6728   151.90308 ]\n",
      " [281.80655  103.733315 340.16272  ... 137.34993  312.13092  136.35306 ]\n",
      " ...\n",
      " [467.8087   140.1361   483.9826   ... 147.57716  475.8387   148.21072 ]\n",
      " [708.1705   292.00662  724.1027   ... 299.9918   716.13477  300.0854  ]\n",
      " [779.80225  140.24034  795.77856  ... 147.69229  787.93787  148.2632  ]]\n",
      "im_detect: 6/6 forward_pass_time: 0.0049s misc: 0.0228s\n",
      "./output_results/5.jpg\n"
     ]
    }
   ],
   "source": [
    "test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848a1e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a863b7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "class MiniRetinaFace(nn.Module):\n",
    "    def __init__(self, cfg = None, phase = 'train'):\n",
    "\n",
    "        super(MiniRetinaFace,self).__init__()\n",
    "        self.phase = phase\n",
    "        backbone = MobileNetV1() \n",
    "        self.body = _utils.IntermediateLayerGetter(backbone, cfg['return_layers'])\n",
    "        in_channels_stage2 = cfg['in_channel']\n",
    "        in_channels_list = [\n",
    "            in_channels_stage2 * 2,\n",
    "            in_channels_stage2 * 4,\n",
    "            in_channels_stage2 * 8,\n",
    "        ]\n",
    "        out_channels = cfg['out_channel']\n",
    "        self.fpn = FPN(in_channels_list,out_channels)\n",
    "        self.ssh1 = SSH(out_channels, out_channels)\n",
    "\n",
    "        self.ClassHead = self._make_class_head(fpn_num=3, inchannels=cfg['out_channel'])\n",
    "        self.BboxHead = self._make_bbox_head(fpn_num=3, inchannels=cfg['out_channel'])\n",
    "        self.LandmarkHead = self._make_landmark_head(fpn_num=3, inchannels=cfg['out_channel'])\n",
    "\n",
    "    def _make_class_head(self,fpn_num=3,inchannels=64,anchor_num=2):\n",
    "        classhead = nn.ModuleList()\n",
    "        for i in range(fpn_num):\n",
    "            classhead.append(ClassHead(inchannels,anchor_num))\n",
    "        return classhead\n",
    "    \n",
    "    def _make_bbox_head(self,fpn_num=3,inchannels=64,anchor_num=2):\n",
    "        bboxhead = nn.ModuleList()\n",
    "        for i in range(fpn_num):\n",
    "            bboxhead.append(BboxHead(inchannels,anchor_num))\n",
    "        return bboxhead\n",
    "\n",
    "    def _make_landmark_head(self,fpn_num=3,inchannels=64,anchor_num=2):\n",
    "        landmarkhead = nn.ModuleList()\n",
    "        for i in range(fpn_num):\n",
    "            landmarkhead.append(LandmarkHead(inchannels,anchor_num))\n",
    "        return landmarkhead\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        out = self.body(inputs)\n",
    "\n",
    "        # FPN\n",
    "        fpn = self.fpn(out)\n",
    "\n",
    "        # SSH\n",
    "        feature1 = self.ssh1(fpn[0])\n",
    "        features = [feature1]\n",
    "\n",
    "        bbox_regressions = torch.cat([self.BboxHead[i](feature) for i, feature in enumerate(features)], dim=1)\n",
    "        classifications = torch.cat([self.ClassHead[i](feature) for i, feature in enumerate(features)],dim=1)\n",
    "        ldm_regressions = torch.cat([self.LandmarkHead[i](feature) for i, feature in enumerate(features)], dim=1)\n",
    "\n",
    "        if self.phase == 'train':\n",
    "            output = (bbox_regressions, classifications, ldm_regressions)\n",
    "        else:\n",
    "            output = (bbox_regressions, F.softmax(classifications, dim=-1), ldm_regressions)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18a6fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.retinaface import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b830fb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing net...\n",
      "MiniRetinaFace(\n",
      "  (body): IntermediateLayerGetter(\n",
      "    (stage1): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
      "        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
      "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (stage2): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (stage3): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fpn): FPN(\n",
      "    (output1): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (output2): Sequential(\n",
      "      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (output3): Sequential(\n",
      "      (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (merge1): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (merge2): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (ssh1): SSH(\n",
      "    (conv3X3): Sequential(\n",
      "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (conv5X5_1): Sequential(\n",
      "      (0): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (conv5X5_2): Sequential(\n",
      "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (conv7X7_2): Sequential(\n",
      "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (conv7x7_3): Sequential(\n",
      "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (ClassHead): ModuleList(\n",
      "    (0): ClassHead(\n",
      "      (conv1x1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (1): ClassHead(\n",
      "      (conv1x1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (2): ClassHead(\n",
      "      (conv1x1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (BboxHead): ModuleList(\n",
      "    (0): BboxHead(\n",
      "      (conv1x1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (1): BboxHead(\n",
      "      (conv1x1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (2): BboxHead(\n",
      "      (conv1x1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (LandmarkHead): ModuleList(\n",
      "    (0): LandmarkHead(\n",
      "      (conv1x1): Conv2d(64, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (1): LandmarkHead(\n",
      "      (conv1x1): Conv2d(64, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (2): LandmarkHead(\n",
      "      (conv1x1): Conv2d(64, 20, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mini_net = MiniRetinaFace(cfg=cfg)\n",
    "print(\"Printing net...\")\n",
    "print(mini_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8df2f5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniRetinaFace(\n",
       "  (body): IntermediateLayerGetter(\n",
       "    (stage1): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
       "        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (3): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (3): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (3): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (stage2): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (3): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (stage3): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fpn): FPN(\n",
       "    (output1): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (output2): Sequential(\n",
       "      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (output3): Sequential(\n",
       "      (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (merge1): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (merge2): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (ssh1): SSH(\n",
       "    (conv3X3): Sequential(\n",
       "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv5X5_1): Sequential(\n",
       "      (0): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv5X5_2): Sequential(\n",
       "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv7X7_2): Sequential(\n",
       "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "    )\n",
       "    (conv7x7_3): Sequential(\n",
       "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (ClassHead): ModuleList(\n",
       "    (0): ClassHead(\n",
       "      (conv1x1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (1): ClassHead(\n",
       "      (conv1x1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (2): ClassHead(\n",
       "      (conv1x1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (BboxHead): ModuleList(\n",
       "    (0): BboxHead(\n",
       "      (conv1x1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (1): BboxHead(\n",
       "      (conv1x1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (2): BboxHead(\n",
       "      (conv1x1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (LandmarkHead): ModuleList(\n",
       "    (0): LandmarkHead(\n",
       "      (conv1x1): Conv2d(64, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (1): LandmarkHead(\n",
       "      (conv1x1): Conv2d(64, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (2): LandmarkHead(\n",
       "      (conv1x1): Conv2d(64, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\" if 0 else \"cuda\")\n",
    "mini_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "893bb087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    net.train()\n",
    "    epoch = 0 + resume_epoch\n",
    "    print('Loading Dataset...')\n",
    "\n",
    "    dataset = DataFaceDetection( training_dataset,preproc(img_dim, rgb_mean))\n",
    "\n",
    "    epoch_size = math.ceil(len(dataset) / batch_size)\n",
    "    max_iter = max_epoch * epoch_size\n",
    "\n",
    "    stepvalues = (cfg['decay1'] * epoch_size, cfg['decay2'] * epoch_size)\n",
    "    step_index = 0\n",
    "\n",
    "    if resume_epoch > 0:\n",
    "        start_iter = resume_epoch * epoch_size\n",
    "    else:\n",
    "        start_iter = 0\n",
    "\n",
    "    for iteration in range(start_iter, max_iter):\n",
    "        if iteration % epoch_size == 0:\n",
    "            # create batch iterator\n",
    "            batch_iterator = iter(data.DataLoader(dataset, batch_size, shuffle=True, num_workers=num_workers, collate_fn=detection_collate))\n",
    "            if (epoch % 10 == 0 and epoch > 0) or (epoch % 5 == 0 and epoch > cfg['decay1']):\n",
    "                torch.save(net.state_dict(), save_folder + cfg['name']+ '_epoch_' + str(epoch) + '.pth')\n",
    "            epoch += 1\n",
    "\n",
    "        load_t0 = time.time()\n",
    "        if iteration in stepvalues:\n",
    "            step_index += 1\n",
    "        lr = adjust_learning_rate(optimizer, gamma, epoch, step_index, iteration, epoch_size)\n",
    "\n",
    "        # load train data\n",
    "        images, targets = next(batch_iterator)\n",
    "        images = images.cuda()\n",
    "        targets = [anno.cuda() for anno in targets]\n",
    "\n",
    "        # forward\n",
    "        out = net(images)\n",
    "\n",
    "        #print (\"Outputs from the model \\n\", out)\n",
    "        print (\"Outputs from the model \\n\", len (out[0]), len (out[0][0]), out[0][0].shape, out[0][0][1].shape)\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss_l, loss_c, loss_landm = criterion(out, priors, targets)\n",
    "        loss = cfg['loc_weight'] * loss_l + loss_c + loss_landm\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        load_t1 = time.time()\n",
    "        batch_time = load_t1 - load_t0\n",
    "        eta = int(batch_time * (max_iter - iteration))\n",
    "        print('Epoch:{}/{} || Epochiter: {}/{} || Iter: {}/{} || Loc: {:.4f} Cla: {:.4f} Landm: {:.4f} || LR: {:.8f} || Batchtime: {:.4f} s || ETA: {}'\n",
    "              .format(epoch, max_epoch, (iteration % epoch_size) + 1,\n",
    "              epoch_size, iteration + 1, max_iter, loss_l.item(), loss_c.item(), loss_landm.item(), lr, batch_time, str(datetime.timedelta(seconds=eta))))\n",
    "\n",
    "    print (\"model save path : \", save_folder + cfg['name'] + '_Final.pth')\n",
    "    torch.save(net.state_dict(), save_folder + cfg['name'] + '_Final.pth')\n",
    "    # torch.save(net.state_dict(), save_folder + 'Final_Retinaface.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2883e4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset...\n",
      "Outputs from the model \n",
      " 16 16800 torch.Size([16800, 4]) torch.Size([4])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [37], line 46\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m loss_l, loss_c, loss_landm \u001b[38;5;241m=\u001b[39m criterion(out, priors, targets)\n\u001b[1;32m     45\u001b[0m loss \u001b[38;5;241m=\u001b[39m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloc_weight\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m loss_l \u001b[38;5;241m+\u001b[39m loss_c \u001b[38;5;241m+\u001b[39m loss_landm\n\u001b[0;32m---> 46\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     48\u001b[0m load_t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88361a12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
